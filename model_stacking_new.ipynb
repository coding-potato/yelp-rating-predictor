{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('we have',ratings.shape[0], 'ratings')\n",
    "print('the number of unique users we have is:', len(ratings.user_id.unique()))\n",
    "print('the number of unique business we have is:', len(ratings.business_id.unique()))\n",
    "print(\"The median user rated %d books.\"%ratings.user_id.value_counts().median())\n",
    "print('The max rating is: %d'%ratings.rating.max(),\"the min rating is: %d\"%ratings.rating.min())\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7ae7ba7ef310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import NormalPredictor\n",
    "from surprise import BaselineOnly, SVD, KNNBasic\n",
    "from surprise.prediction_algorithms.co_clustering import CoClustering\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import split\n",
    "\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "review_datafile = os.path.join(DATA_FOLDER,\"review.json\")\n",
    "line_count = len(open(review_datafile).readlines())\n",
    "user_ids, business_ids, stars, dates = [], [], [], []\n",
    "with open(review_datafile) as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        user_ids += [blob[\"user_id\"]]\n",
    "        business_ids += [blob[\"business_id\"]]\n",
    "        stars += [blob[\"stars\"]]\n",
    "        dates += [blob[\"date\"]]\n",
    "ratings = pd.DataFrame(\n",
    "   {\"user_id\": user_ids, \"business_id\": business_ids, \"rating\": stars, \"date\": dates}\n",
    ")\n",
    "user_counts = ratings[\"user_id\"].value_counts()\n",
    "active_users = user_counts.loc[user_counts >= 5].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings[['user_id', 'business_id', 'rating']], reader)\n",
    "train_set, test_set = train_test_split(data, test_size=.2)\n",
    "kSplit = split.KFold(n_splits=2, shuffle=True) # split data into folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. find the best hyperparameters for BaselineOnly\n",
    "param_grid = {'bsl_options': {'method': ['als','sgd'],\n",
    "                              'n_epochs': [10, 25], \n",
    "                              'reg_u': [3, 5],\n",
    "                              'reg_i': [3, 5]}\n",
    "             }\n",
    "grid_search = GridSearchCV(BaselineOnly, param_grid, measures=['rmse'], cv=3)\n",
    "grid_search.fit(data)\n",
    "print(grid_search.best_score['rmse'])\n",
    "print(grid_search.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. find the best hyperparameters for SVD\n",
    "param_grid = {'n_epochs': [25, 40], 'lr_all': [0.01, 0.02],\n",
    "              'reg_all': [0.2]}\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "grid_search.fit(data)\n",
    "print(grid_search.best_score['rmse'])\n",
    "print(grid_search.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. find the best hyperparameters for co-clustering\n",
    "param_grid = {'n_epochs': [3, 5], 'n_cltr_u': [3, 5],\n",
    "              'n_cltr_i': [3, 5]}\n",
    "grid_search = GridSearchCV(CoClustering, param_grid, measures=['rmse'], cv=3)\n",
    "grid_search.fit(data)\n",
    "print(grid_search.best_score['rmse'])\n",
    "print(grid_search.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmseBaseline = []\n",
    "rmseSVD = []\n",
    "rmseCo = []\n",
    "rmseSlope = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineOnly = BaselineOnly(bsl_options={'method': 'als',\n",
    "                                   'n_epochs': 25,\n",
    "                                   'reg_u': 5,\n",
    "                                   'reg_i': 3\n",
    "                                   })\n",
    "for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "    baselineOnly.fit(trainset)\n",
    "    predictionsBaselineOnly = baselineOnly.test(testset)\n",
    "    rmseBaseline.append(accuracy.rmse(predictionsBaselineOnly,verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = SVD(lr_all=0.01,n_epochs=25,reg_all=0.2)\n",
    "for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "    svd.fit(trainset)\n",
    "    predictionsSVD = svd.test(testset)\n",
    "    rmseSVD.append(accuracy.rmse(predictionsSVD,verbose=True))#get root means squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coClus = CoClustering(n_epochs=3, n_cltr_u=3, n_cltr_i=3) \n",
    "for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "    coClus.fit(trainset)\n",
    "    predictionsCoClus = coClus.test(testset)\n",
    "    rmseCo.append(accuracy.rmse(predictionsCoClus,verbose=True))#get root means squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise.prediction_algorithms.slope_one import SlopeOne\n",
    "# slopeOne = SlopeOne()\n",
    "# for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "#     slopeOne.fit(trainset)\n",
    "#     predictionsSlope = slopeOne.test(testset)\n",
    "#     rmseSlope.append(accuracy.rmse(predictionsSlope,verbose=True))#get root means squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase\n",
    "import numpy as np\n",
    "class HybridFacto(AlgoBase):\n",
    "    def __init__(self,epochs, learning_rate,num_models):\n",
    "        self.alpha = np.array([1.0/num_models]*num_models)\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "    def fit(self,holdout):\n",
    "        holdout = holdout.build_full_trainset().build_testset()\n",
    "        for epoch in range(self.epochs):\n",
    "            predictions = np.array([baselineOnly.test(holdout),svd.test(holdout),coClus.test(holdout)])\n",
    "            maeGradient = [accuracy.rmse(prediction,verbose=True) for prediction in predictions]\n",
    "            newalpha = self.alpha - learning_rate * maeGradient  \n",
    "            #convergence check:\n",
    "            if newalpha - self.alpha < 0.001:\n",
    "                break\n",
    "            self.alpha = newalpha\n",
    "    def estimate(self,u,i):\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unkown.')\n",
    "        algoResults = np.array([collabKNN.predict(u,i),funkSVD.predict(u,i),coClus.predict(u,i)])\n",
    "        return np.sum(np.dot(self.alpha,algoResults))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid = HybridFacto(epochs = 10,learning_rate = 0.05,num_models = 3)\n",
    "hybrid.fit(data)\n",
    "rmseHyb = []\n",
    "for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "    predhybrid = Hyhybrid.test(testset)\n",
    "    rmseHyb.append(accuracy.rmse(predhybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_options = sim_options = {'name': 'cosine',\n",
    "#                'user_based': False  # compute  similarities between items\n",
    "#                }\n",
    "# collabKNN = KNNBasic(k=40,sim_options=sim_options) #try removing sim_options. You'll find memory errors. \n",
    "# collabKNN = KNNBasic(k=40)\n",
    "baseline_als = BaselineOnly(bsl_options={'method': 'als',\n",
    "                                   'n_epochs': 5,\n",
    "                                   'reg_u': 12,\n",
    "                                   'reg_i': 5\n",
    "                                   })\n",
    "rmseKNN = []\n",
    "rmseSVD = []\n",
    "rmseCo = []\n",
    "rmseSlope = []\n",
    "for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "    baseline_als.fit(trainset)\n",
    "    predictionsKNN = baseline_als.test(testset)\n",
    "    rmseKNN.append(accuracy.rmse(predictionsKNN,verbose=True))#get root means squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms.co_clustering import CoClustering\n",
    "coClus = CoClustering(n_cltr_u=4,n_cltr_i=4,n_epochs=25) \n",
    "for trainset, testset in kSplit.split(data): #iterate through the folds.\n",
    "    coClus.fit(trainset)\n",
    "    predictionsCoClus = coClus.test(testset)\n",
    "    rmseCo.append(accuracy.rmse(predictionsCoClus,verbose=True))#get root means squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {'Baseline_ALS': [BaselineOnly(bsl_options={'method': 'als',\n",
    "                                   'n_epochs': 5,\n",
    "                                   'reg_u': 12,\n",
    "                                   'reg_i': 5\n",
    "                                   }),]\n",
    "        }\n",
    "\n",
    "\n",
    "# algos = {'Baseline_ALS': BaselineOnly(bsl_options={'method': 'als',\n",
    "#                                    'n_epochs': 5,\n",
    "#                                    'reg_u': 12,\n",
    "#                                    'reg_i': 5\n",
    "#                                    }),\n",
    "#          'Baseline_SGD': BaselineOnly(bsl_options={'method': 'sgd',\n",
    "#                                    'learning_rate': .00005,\n",
    "#                                    }),\n",
    "#          'SVD': SVD(),\n",
    "#          'NeighborhoodBased_User': KNNBasic(sim_options={'name': 'cosine',\n",
    "#                                'user_based': True\n",
    "#                                })\n",
    "#         }\n",
    "algo_list = {}\n",
    "for algo_name in algos:\n",
    "    print (f\"Algo is: {algo_name}\")\n",
    "    algo = algos[algo_name]\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    algo_accuracy = accuracy.rmse(predictions)\n",
    "    algos[algo_name][1] = algo_accuracy\n",
    "    print (algo_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yelp)",
   "language": "python",
   "name": "yelp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
