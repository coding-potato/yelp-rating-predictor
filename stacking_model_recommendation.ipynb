{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble model\n",
    "\n",
    "\n",
    "The idea of ensemble model is to stack a set of base recommendation models and use the weighted sum of the predictions as the new prediction which will  remove the flaws in individual implementation.\n",
    "\n",
    "### Steps:\n",
    "1. Tune the hyperparameter of base models including:\n",
    "    - Neighborhood based model\n",
    "    - SVD\n",
    "    - baselineonly\n",
    "    - co-colustering\n",
    "2. Train base models using the tuned hyperparameters\n",
    "3. Stack models together with weights\n",
    "4. Run linear regression to learn the weights\n",
    "\n",
    "### Results:\n",
    "Accuracy matrix: RMSE\n",
    "Stacking Model (0.69) vs Base Models (1.29 - 1.51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6685900/6685900 [00:55<00:00, 121218.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import NormalPredictor\n",
    "from surprise import BaselineOnly, SVD, KNNBasic\n",
    "from surprise.prediction_algorithms.co_clustering import CoClustering\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import split\n",
    "\n",
    "\n",
    "DATA_FOLDER = 'yelp_dataset'\n",
    "review_datafile = os.path.join(DATA_FOLDER,\"review.json\")\n",
    "line_count = len(open(review_datafile).readlines())\n",
    "user_ids, business_ids, stars, dates = [], [], [], []\n",
    "with open(review_datafile) as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        user_ids += [blob[\"user_id\"]]\n",
    "        business_ids += [blob[\"business_id\"]]\n",
    "        stars += [blob[\"stars\"]]\n",
    "        dates += [blob[\"date\"]]\n",
    "ratings = pd.DataFrame(\n",
    "   {\"user_id\": user_ids, \"business_id\": business_ids, \"rating\": stars, \"date\": dates}\n",
    ")\n",
    "user_counts = ratings[\"user_id\"].value_counts()\n",
    "active_users = user_counts.loc[user_counts >= 5].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample of 1M datapoints\n",
    "ratings_sample = ratings[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings_sample[['user_id', 'business_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "1.2732091440616835\n",
      "{'bsl_options': {'method': 'als', 'n_epochs': 25, 'reg_u': 5, 'reg_i': 3}}\n"
     ]
    }
   ],
   "source": [
    "# 1. find the best hyperparameters for BaselineOnly\n",
    "param_grid = {'bsl_options': {'method': ['als','sgd'],\n",
    "                              'n_epochs': [10, 25], \n",
    "                              'reg_u': [3, 5],\n",
    "                              'reg_i': [3, 5]}\n",
    "             }\n",
    "grid_search = GridSearchCV(BaselineOnly, param_grid, measures=['rmse'], cv=3)\n",
    "grid_search.fit(data)\n",
    "print(grid_search.best_score['rmse'])\n",
    "print(grid_search.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2783104771201215\n",
      "{'n_epochs': 25, 'lr_all': 0.01, 'reg_all': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# 2. find the best hyperparameters for SVD\n",
    "param_grid = {'n_epochs': [25, 40], 'lr_all': [0.01, 0.02],\n",
    "              'reg_all': [0.2]}\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "grid_search.fit(data)\n",
    "print(grid_search.best_score['rmse'])\n",
    "print(grid_search.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4121145162238307\n",
      "{'n_epochs': 3, 'n_cltr_u': 3, 'n_cltr_i': 3}\n"
     ]
    }
   ],
   "source": [
    "# 3. find the best hyperparameters for co-clustering\n",
    "param_grid = {'n_epochs': [3, 5], 'n_cltr_u': [3, 5],\n",
    "              'n_cltr_i': [3, 5]}\n",
    "grid_search = GridSearchCV(CoClustering, param_grid, measures=['rmse'], cv=3)\n",
    "grid_search.fit(data)\n",
    "print(grid_search.best_score['rmse'])\n",
    "print(grid_search.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.stacking_recommender import StackingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_model = StackingModel(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** Start retraining models ******************\n",
      "*************** Retraining: baselineonly *****************\n",
      "Estimating biases using als...\n",
      "RMSE: 1.2862\n",
      "Estimating biases using als...\n",
      "RMSE: 1.2883\n",
      "Estimating biases using als...\n",
      "RMSE: 1.2930\n",
      "Estimating biases using als...\n",
      "RMSE: 1.2895\n",
      "Estimating biases using als...\n",
      "RMSE: 1.2891\n",
      "Estimating biases using als...\n",
      "RMSE: 1.2854\n",
      "Estimating biases using als...\n",
      "RMSE: 1.2909\n",
      "Estimating biases using als...\n",
      "RMSE: 1.2906\n",
      "Estimating biases using als...\n",
      "RMSE: 1.2869\n",
      "Estimating biases using als...\n",
      "RMSE: 1.2934\n",
      "*************** Retraining: svd *****************\n",
      "RMSE: 1.2951\n",
      "RMSE: 1.2942\n",
      "RMSE: 1.2880\n",
      "RMSE: 1.2912\n",
      "RMSE: 1.3016\n",
      "RMSE: 1.2900\n",
      "RMSE: 1.2938\n",
      "RMSE: 1.2967\n",
      "RMSE: 1.2912\n",
      "RMSE: 1.2953\n",
      "*************** Retraining: coClustering *****************\n",
      "RMSE: 1.4794\n",
      "RMSE: 1.4815\n",
      "RMSE: 1.4791\n",
      "RMSE: 1.4834\n",
      "RMSE: 1.4718\n",
      "RMSE: 1.4802\n",
      "RMSE: 1.4792\n",
      "RMSE: 1.4741\n",
      "RMSE: 1.4804\n",
      "RMSE: 1.4769\n",
      "*************** Retraining: knn *****************\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.5215\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.5157\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.5216\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.5158\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.5204\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.5154\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.5137\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.5159\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.5167\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.5147\n",
      "******************** Models retrained *********************\n",
      "*** Starting tuning hyperparameter for stacking weights ***\n"
     ]
    }
   ],
   "source": [
    "stacking_model.fit(data, retrain=True, retrain_split_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6910324929352037"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_model_pred = stacking_model.test(data.build_full_trainset().build_testset())\n",
    "accuracy.rmse(stacking_model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
